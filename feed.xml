<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DevMine</title>
    <description>DevMine - A developer search engine and source code analysis framework
</description>
    <link>http://devmine.ch/</link>
    <atom:link href="http://devmine.ch/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 12 Jun 2015 10:49:08 +0200</pubDate>
    <lastBuildDate>Fri, 12 Jun 2015 10:49:08 +0200</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Recent performance and reliability improvements</title>
        <description>&lt;p&gt;Our main focus for the past months has been on improving the existing tools.
This technical article talks about some of the steps we took to improve
both the quality and reliability of some of the DevMine tools and more
specifically about the improvements made on &lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt; and
&lt;a href=&quot;/doc/repotool&quot;&gt;repotool&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Back in the last months of 2014, our focus on &lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt; has mainly
been on the crawlers part. The simple explanation is that the fetcher part was
good enough for the amount of data we had back then. We knew some parts needed
improvements like, for instance, the fetcher not being able to update a git
repository in a detached head state. However, performance was generally not an
issue. This hold true until &lt;a href=&quot;/doc/ght2dm&quot;&gt;ght2dm&lt;/a&gt; came into the game.&lt;/p&gt;

&lt;p&gt;This spring, with &lt;a href=&quot;/doc/ght2dm&quot;&gt;ght2dm&lt;/a&gt;, we started importing data from the
&lt;a href=&quot;http://ghtorrent.org/&quot;&gt;GHTorrent&lt;/a&gt; project. Suddenly, from the mere tens of
thousands repositories metadata we had, we ended up having millions. It is one
thing to clone and update a few thousands repositories, it is another when it
comes to hundreds of thousands.&lt;/p&gt;

&lt;h2 id=&quot;performance-improvements&quot;&gt;Performance improvements&lt;/h2&gt;

&lt;h3 id=&quot;use-tar-archives-when-storing-source-code-repositories&quot;&gt;Use tar archives when storing source code repositories&lt;/h3&gt;

&lt;p&gt;As a first step, we cloned approximately 300’000 source code repositories. This
was a huge bump from the ~80’000 we had clone so far and we started having some
issues with the file system. For instance, we had no way to tell how large was
the directory in which we clone the repositories. Why? Because there was already
so many files that even after running for a week, &lt;code&gt;du(1)&lt;/code&gt; would not return a
result… Something clearly had to be done…&lt;/p&gt;

&lt;p&gt;At this point, we decided that storing too many files was not an option. Hence,
the decision was made to archive all source code repositories in tar format.
This meant that all the language parsers, &lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt; and
&lt;a href=&quot;/doc/repotool&quot;&gt;repotool&lt;/a&gt; had to be adapted to the change. For the language
parsers, the change was fairly easy: they simply have to be able to read the
content of a tar archive. However, it was another story for
&lt;a href=&quot;/doc/repotool&quot;&gt;repotool&lt;/a&gt; and something even more challenging for
&lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/doc/repotool&quot;&gt;repotool&lt;/a&gt; uses &lt;a href=&quot;https://github.com/libgit2/git2go&quot;&gt;git2go&lt;/a&gt;, a
Go binding to &lt;a href=&quot;https://libgit2.github.com&quot;&gt;libgit2&lt;/a&gt;, a C library which
implements the git protocol. Since it does not work on tar archives, the only
way to get commits information is by extracting the archive, at least the &lt;code&gt;.git&lt;/code&gt;
directory of the git repository, because this is where repository information is
stored. This means that &lt;a href=&quot;/doc/repotool&quot;&gt;repotool&lt;/a&gt; has to extract a tar archive
before being able to read the repository information it looks for and, of
course, delete the extracted content afterwards.&lt;/p&gt;

&lt;p&gt;In the same vein, &lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt; has to extract a repository archive
before being able to perform an update. Of course, if the tar option is
activated, it has to re-create the tar archive after the operation. Upon new
repository clone, it has to create a tar archive as well.&lt;/p&gt;

&lt;p&gt;So this meant lots of changes but in the end, all the work was well worth it.
&lt;code&gt;du(1)&lt;/code&gt; is now quick to calculate the size of the clone directory even with more
than 10TB of data. Having 1 file per repository instead of tens of thousands is
really a game changer. We also noticed a considerable speedup when parsing
source code from repositories. Indeed, only one &lt;code&gt;open&lt;/code&gt;/&lt;code&gt;close&lt;/code&gt; file operation is
now required per repository in order to parse the code. This changes
everything…&lt;/p&gt;

&lt;p&gt;On the other hand, having to deal with tar archives brings a little overhead to
&lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt; and &lt;a href=&quot;/doc/repotool&quot;&gt;repotool&lt;/a&gt; operations. One of the
next paragraph explains how we dealt with this matter.&lt;/p&gt;

&lt;h3 id=&quot;parallelize-the-tasks&quot;&gt;Parallelize the tasks&lt;/h3&gt;

&lt;p&gt;Until recently, only two goroutines were spawned by &lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt;: one
to crawl GitHub repositories, users and organizations metadata and another one
to fetch git repositories. As there is no hard dependency between two different
repositories, there is no reason not to use several goroutines for the fetching
part. So now, the user has the possibility to set the number of goroutines to
use to fetch repositories. This is especially an improvement now that there is
an induced overhead when storing source code in tar archives.&lt;/p&gt;

&lt;p&gt;In the past, we used to use a script which was iterating over the repositories
giving repositories path location to a number of goroutines which were spawning
&lt;a href=&quot;/doc/repotool&quot;&gt;repotool&lt;/a&gt; processes to collect commits information.
It used to be decent enough in that it took about 2 weeks to collect ~15mio
commits and ~150mio commit diff deltas from the ~80’000 repositories and store
them into the database. This way of doing allowed &lt;code&gt;repotool&lt;/code&gt; to run in parallel
but at the cost of spawning a lot of processes.&lt;/p&gt;

&lt;h3 id=&quot;avoid-spawning-processes&quot;&gt;Avoid spawning processes&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt; used to spawn git processes to clone and update
repositories. This worked okay for a moment but it is not viable when you need
to scale. Consider that a process is spawned each time we do a clone or update
operation on a repository, that throw-away process creation is costly and that
we now have millions of repositories to deal with. Hence, we decided to use
&lt;a href=&quot;https://libgit2.github.com&quot;&gt;libgit2&lt;/a&gt; via
&lt;a href=&quot;https://github.com/libgit2/git2go&quot;&gt;git2go&lt;/a&gt; which we were already using with
&lt;a href=&quot;/doc/repotool&quot;&gt;repotool&lt;/a&gt; with satisfying results.&lt;/p&gt;

&lt;p&gt;Using this library allows us to take some shortcuts on repository update
operation. At first, one might think that it shall mimic the &lt;code&gt;git pull&lt;/code&gt; command,
ie doing the equivalent of &lt;code&gt;git fetch &amp;amp;&amp;amp; git merge FETCH_HEAD&lt;/code&gt;. However,
repositories, in our context, are only meant to be read from. Hence, there shall
never be anything to actually merge. So the update operation only fetches
changes from remote and fast-forwards locally which avoids doing an actual
merge.&lt;/p&gt;

&lt;p&gt;In the previous section, I mentioned that we were using a script to run several
instances of &lt;a href=&quot;/doc/repotool/&quot;&gt;repotool&lt;/a&gt; in parallel. Now, this means spawning
hundreds of thousands of processes to collect commits information from locally
stored repositories. Again, this used to work fine with a limited number of
repositories. Now that we have ~750’000 git repositories clone (and that it is
just small fraction of what we aim at collecting), it is a problem.  Hence, we
split &lt;a href=&quot;/doc/repotool&quot;&gt;repotool&lt;/a&gt; into two binaries, one to output information as
JSON which works like &lt;a href=&quot;/doc/repotool&quot;&gt;repotool&lt;/a&gt; always has, and another one,
&lt;code&gt;repotool-db&lt;/code&gt;, which is responsible for inserting information into the database.
Now, instead of spawning costly process, it uses goroutine which is way lighter.
Also, we decided that inserting commit diff deltas was too much and we simply do
not collect them into the database anymore. &lt;code&gt;repotool-db&lt;/code&gt; is still capable of
doing it but it is slower and generates a very large quantity of data. We were
also able to speed things up by inserting commits information in batches of 1mio
per transaction, having a routine which takes care of that, instead of one
transaction per commit. By not storing commit diff deltas, we are also now able
to disable foreign keys constraints and indexes which speed things up. In order
to associate commits information with repositories and users, we also used to
query the database 1 time per project to retrieve the repository ID and two
times per commit to get the author and committer IDs. You can guess that this
was impacting performances by a lot. Instead now, all repositories and users IDs
are now queried once when &lt;code&gt;repotool-db&lt;/code&gt; starts and stored in a hashmap for a
O(1) access time.&lt;/p&gt;

&lt;h3 id=&quot;use-a-ramdisk&quot;&gt;Use a ramdisk&lt;/h3&gt;

&lt;p&gt;Storing source code repositories as tar archives has several advantages as I
mentioned in a previous section. This does however bring some overhead for the
repositories update operation. Actually, &lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt; now appeared to
be limited by disk I/Os, without a surprise. Something had to be done and the
ideal solution for this case is to use a ramdisk. The only thing we care to
store is only the tar archive of the repositories after all. So
&lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt; has been modified to use a temporary directory, which can
be specified by the user, to extract tar archives in order to update the
repositories. After the update, repositories are simply re-tar’ed in their
original location. There is no reasons not to clone repositories in ramdisk as
well and create the tar archive on main storage afterwards. The only problem
which may arise by doing so is if the repository does not fit in size in the
ramdisk. Most of repositories are not very large so &lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt;’s
behavior is to attempt at cloning in the ramdisk and fallback to cloning
directly on main storage if that reveals to be impossible. By doing so, we
uncovered &lt;a href=&quot;https://github.com/libgit2/libgit2/pull/3174&quot;&gt;a bug&lt;/a&gt; in
&lt;a href=&quot;https://libgit2.github.com&quot;&gt;libgit2&lt;/a&gt; where attempting to clone a repository in
a not large enough partition leads to a crash because of a SIGBUS signal raised
from an attempt at mapping data somewhere it is not allowed to. Fortunately for
us, &lt;a href=&quot;https://github.com/carlosmn&quot;&gt;Carlos Martín Nieto&lt;/a&gt;, one of the main
&lt;a href=&quot;https://libgit2.github.com&quot;&gt;libgit2&lt;/a&gt; developer was very quick to address that
issue.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/doc/repotool&quot;&gt;repotool&lt;/a&gt; also uses a ramdisk when repositories are stored as
tar archives. Since it has to untar them in order to be able to collect commits
information, this is done in a ramdisk. Note that only the &lt;code&gt;.git&lt;/code&gt; folder is
extracted since this is all that is required to collect the information. Of
course, this temporary directory is simply removed after the operation. Again,
doing this operation in RAM speeds things up considerably.&lt;/p&gt;

&lt;h2 id=&quot;reliability-improvements&quot;&gt;Reliability improvements&lt;/h2&gt;

&lt;h3 id=&quot;take-the-right-decision-when-errors-are-raised&quot;&gt;Take the right decision when errors are raised&lt;/h3&gt;

&lt;p&gt;Handling errors correctly is essential, especially with tools that are supposed
to run continuously for long period of times. However, it is not always clear at
first what the right decision is to handle some errors.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt; used to skip a repository when it was not able to update
it. Now, when this happens, unless it is due to a network error, it deletes it
from disk and re-clones it. Not doing this when the error is network related is
very important because sometimes repositories are deleted from remote and we do
not want to delete our copy without any possibility to get it back afterwards.&lt;/p&gt;

&lt;h3 id=&quot;restart-where-interrupted-after-a-crash&quot;&gt;Restart where interrupted after a crash&lt;/h3&gt;

&lt;p&gt;Even with all our efforts to make the tools stable, crashes happen. They may be
related to a library error or an unexpected non-frequent condition, fact is that
they do happen. When the tool has been running for a few days, you are usually
quite sad if when checking if it still is running and you note that it has
stopped running and, moreover, that this means it has to start over from the
start again.&lt;/p&gt;

&lt;p&gt;This is why &lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt; now remembers, using a file, the ID of the
last successfully processed repository. Hence, after a crash or having been
stopped on a voluntary basis, it can resume its operations where it stopped.&lt;/p&gt;

&lt;h3 id=&quot;use-an-error-rate-based-throttler&quot;&gt;Use an error rate based throttler&lt;/h3&gt;

&lt;p&gt;When a tool runs continuously for very long period of time, like
&lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt;, you usually simply deal with an error and log it. This is
usually fine but think about how the tool would behave if the storage space is
full or the network is down? Yes, &lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt; would record &lt;em&gt;a lot&lt;/em&gt; of
errors and try to keep on going… This is obviously not a desired behavior. Our
take on this problem was to implement an error rate based throttler. What it
does now is that each error happening is not only dealt with and logged but it
is also recorded by the error rate based throttler. When the error rates gets
too high, and all the parameters can be adjusted by the user, which means that
something really wrong is probably happening, it automatically throttles all
routines for a predefined period of time. In consequence, &lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt;
pauses operations and when the administrator resolves the underlying problem,
such as freeing space on the storage device or restoring the network, all
operations are resumed.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this article, I mentioned several steps taken to improve performances and
reliability of &lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt; and &lt;a href=&quot;/doc/repotool&quot;&gt;repotool&lt;/a&gt;. Nothing much
can be measured with regard to stability improvements but I can simply say that
things are now way better. However, to assess my performance improvement claims,
I shall provide some numbers.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;/doc/repotool&quot;&gt;repotool&lt;/a&gt; changes allow us to now insert ~20-25mio commits
per day on our machine (currently a server with an Intel Xeon CPU E5-2630L v2 @
2.40GHz processor, 128GB or ECC RAM and 20TB of storage in a 6 times 4TB RAID 5
configuration), which is a big improvement comparing to the ~15mio in two weeks
previously. To be completely fair, we do not insert commit diff deltas anymore
but we gained a huge speedup nonetheless.&lt;/p&gt;

&lt;p&gt;As for &lt;a href=&quot;/doc/crawld&quot;&gt;crawld&lt;/a&gt;, we have no real numbers to compare with. However,
I can say that we are now able to collect up to 8 times more data in a single
day than we used to collect in a week. With our server and infrastructure, we
are now able to clone/update ~6-8TB of repositories data per day.&lt;/p&gt;
</description>
        <pubDate>Wed, 10 Jun 2015 00:00:00 +0200</pubDate>
        <link>http://devmine.ch/news/2015/06/10/performance-and-reliability-improvements/</link>
        <guid isPermaLink="true">http://devmine.ch/news/2015/06/10/performance-and-reliability-improvements/</guid>
        
        
        <category>technical</category>
        
      </item>
    
      <item>
        <title>srcanlzr presentation video</title>
        <description>&lt;p&gt;
    Here is a video that present the tool
    &lt;a href=&quot;/doc/srcanlzr&quot;&gt;&lt;code&gt;srcanlzr&lt;/code&gt;&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;
    &lt;a href=&quot;/doc/srcanlzr&quot;&gt;&lt;code&gt;srcanlzr&lt;/code&gt;&lt;/a&gt; is a source code analyzer.
    It analyzes a generic JSON representation of the source code, as generated
    by a source code language parser (only Go and Java parsers exist for now).
&lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/130125936?color=337ab7&quot; width=&quot;800&quot; height=&quot;450&quot; frameborder=&quot;0&quot; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
</description>
        <pubDate>Mon, 08 Jun 2015 00:00:00 +0200</pubDate>
        <link>http://devmine.ch/news/2015/06/08/srcanlzr-presentation/</link>
        <guid isPermaLink="true">http://devmine.ch/news/2015/06/08/srcanlzr-presentation/</guid>
        
        
        <category>video</category>
        
      </item>
    
      <item>
        <title>crawld presentation video</title>
        <description>&lt;p&gt;
    Here is a video that present the tool
    &lt;a href=&quot;/doc/crawld&quot;&gt;&lt;code&gt;crawld&lt;/code&gt;&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;
    &lt;a href=&quot;/doc/crawld&quot;&gt;&lt;code&gt;crawld&lt;/code&gt;&lt;/a&gt; is a data crawler and a source
    code repository fetcher. It is able to crawl metadata about source code
    repositories and developers from websites such as GitHub. All data is then
    stored into a database.
&lt;/p&gt;
&lt;p&gt;
    Along with the crawler, the fetcher can clone and update source code
    repositories fo further analysis through the devmine&#39;s tools.
&lt;/p&gt;


&lt;iframe src=&quot;https://player.vimeo.com/video/130108744?color=337ab7&quot; width=&quot;800&quot; height=&quot;450&quot; frameborder=&quot;0&quot; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
</description>
        <pubDate>Mon, 08 Jun 2015 00:00:00 +0200</pubDate>
        <link>http://devmine.ch/news/2015/06/08/crawld-presentation/</link>
        <guid isPermaLink="true">http://devmine.ch/news/2015/06/08/crawld-presentation/</guid>
        
        
        <category>video</category>
        
      </item>
    
      <item>
        <title>repotool presentation video</title>
        <description>&lt;p&gt;
    Here is a video that shows the most used features of
    &lt;a href=&quot;/doc/repotool&quot;&gt;&lt;code&gt;repotool&lt;/code&gt;&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;
    &lt;a href=&quot;/doc/repotool&quot;&gt;&lt;code&gt;repotool&lt;/code&gt;&lt;/a&gt; is part
    of the DevMine source code analysis framework. It is a command line tool
    that aggregates source code repositories metadata (such as VCS type, commits
    and so on). It can also be used in addition to the result produced by
    &lt;a href=&quot;/doc/srctool&quot;&gt;&lt;code&gt;srctool&lt;/code&gt;&lt;/a&gt; and output
    a combined JSON object.
&lt;/p&gt;
&lt;p&gt;
    The result may be used as input for the
    &lt;a href=&quot;/doc/srcanlzr&quot;&gt;&lt;code&gt;srcanlzr&lt;/code&gt;&lt;/a&gt;
    tool.
&lt;/p&gt;
&lt;iframe src=&quot;https://player.vimeo.com/video/129416052?color=337ab7&quot; width=&quot;800&quot; height=&quot;450&quot; frameborder=&quot;0&quot; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
</description>
        <pubDate>Mon, 01 Jun 2015 00:00:00 +0200</pubDate>
        <link>http://devmine.ch/news/2015/06/01/repotool-presentation/</link>
        <guid isPermaLink="true">http://devmine.ch/news/2015/06/01/repotool-presentation/</guid>
        
        
        <category>video</category>
        
      </item>
    
      <item>
        <title>Writing a language parser</title>
        <description>&lt;p&gt;In order to analyze source code, the DevMine framework uses programming language
parsers that produce a
&lt;a href=&quot;http://godoc.org/github.com/DevMine/srcanlzr/src&quot;&gt;custom abstract syntax tree&lt;/a&gt;
(&lt;a href=&quot;http://en.wikipedia.org/wiki/Abstract_syntax_tree&quot;&gt;AST&lt;/a&gt;). This article talks
about some of the steps that need to be taken in order to implement a language
parser that is not yet available to the framework.&lt;/p&gt;

&lt;p&gt;The first step is usually finding a parser for the programming language you want
to parse. From a high level point a view, the goal is to transform the AST
returned by this parser into DevMine’s custom AST. This means converting the
fields from the parser’s AST into the corresponding structures of DevMine’s AST,
as long as they exist in the language you are working on.&lt;/p&gt;

&lt;p&gt;The new parser shall output JSON to the standard output. All logs or error
messages are expected to be written to the standard error output. This is the
expected behavior that allows different tools of the framework to be used in
chain via input/output piping. For instance, the output of the language parser
can be piped as input to &lt;a href=&quot;/doc/srctool&quot;&gt;&lt;code&gt;srctool&lt;/code&gt;&lt;/a&gt; which can then pipe its
output into &lt;a href=&quot;/doc/srcanlzr&quot;&gt;&lt;code&gt;srcanlzr&lt;/code&gt;&lt;/a&gt;. Hence, for this to work, only JSON
needs to be produced to the standard output. Even informative messages need to
be sent to standard error output.&lt;/p&gt;

&lt;p&gt;JSON marshalling shall follow the syntactic rules of the
&lt;a href=&quot;http://godoc.org/github.com/DevMine/srcanlzr/src&quot;&gt;documentation&lt;/a&gt; in order for
the JSON merger tools to work properly. In the case of an expression or a
statement, the first attribute of this structure in the JSON representation must
absolutely be the expression name, or the statement name, respectively. The best
way to ensure this is to build structures and their attibutes in the same order
than in the &lt;a href=&quot;http://godoc.org/github.com/DevMine/srcanlzr/src&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are a few more points that require some attention. The first may appear to
be obvious but is still worth mentioning. A parser will only take care of the
source code files that belong to the language it is parsing. If the repository
is made of different programming languages, other parsers will then parse the
project to manage these languages.&lt;/p&gt;

&lt;p&gt;The ‘Project’ structure of the AST is the one that contains all the others.
Basically, the JSON object returned by the parser is a ‘Project’. Its name shall
be the name of the repository.&lt;/p&gt;

&lt;p&gt;Furthermore, one of the attributes of the ‘Project’ structure is a list of
language structures. The parser shall return a list of one element. It is
&lt;a href=&quot;/doc/srcanlzr&quot;&gt;&lt;code&gt;srcanlzr&lt;/code&gt;&lt;/a&gt; which will take care of filling the languages list
when merging different JSON objects from different parsers when appropriate.&lt;/p&gt;

&lt;p&gt;About the ‘Package’ structure, the ‘name’ attribute shall be the same as the
name of the folder that contains source code files. For the specific case where
source code files happen to be at the root of the repository, the name of the
package should then be the same as the repository.&lt;/p&gt;

&lt;p&gt;Different structures have a number of lines of code as attribute. The parser
shall not count imports, comments nor annotations as lines of code. Only
statements and declarations are to be counted. Multilines of code such as in the
following snippet have to be counted as &lt;strong&gt;1&lt;/strong&gt; line.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;String code = new StringBuilder()
           .append(&quot;Peace is a lie, there is only passion.\n&quot;)
           .append(&quot;Through passion, I gain strength.\n&quot;)
           .append(&quot;Through strength, I gain power.\n&quot;)
           .append(&quot;Through power, I gain victory.\n&quot;)
           .append(&quot;Through victory, my chains are broken.\n&quot;)
           .append(&quot;The force shall free me.&quot;)
           .toString();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The language parser will be run from the command line with a path as argument.
This path either refers to a source code repository or to a tar archive of such
a repository. Hence, it has to be able to handle both cases indifferently.&lt;/p&gt;

&lt;p&gt;As the name given to the ‘Project’ structure may come from the path given as
argument when the parser is run, care should be taken not to use a dot when
parsing the current directory for instance, as this would lead to a wrong
result. Absolute paths are preferred to relative ones. In the same vein, when
dealing with a tar archive of repository, the archive’s name trimmed from its
suffix shall be used.&lt;/p&gt;

&lt;p&gt;This article has been written having in mind the pitfall encountered while
writing and testing the DevMine’s current parsers but it may not foresee every
possible difficulties. In any case, always refer to the
&lt;a href=&quot;http://godoc.org/github.com/DevMine/srcanlzr/src&quot;&gt;documentation&lt;/a&gt; in case of
doubt, and feel free to write us an email if you have any questions.&lt;/p&gt;
</description>
        <pubDate>Sun, 31 May 2015 00:00:00 +0200</pubDate>
        <link>http://devmine.ch/news/2015/05/31/how-to-write-a-parser/</link>
        <guid isPermaLink="true">http://devmine.ch/news/2015/05/31/how-to-write-a-parser/</guid>
        
        
        <category>technical</category>
        
      </item>
    
      <item>
        <title>srctool presentation video</title>
        <description>&lt;p&gt;
Here is a short video that shows &lt;a href=&quot;/doc/srctool&quot;&gt;&lt;code&gt;srctool&lt;/code&gt;&lt;/a&gt;
in action. This is the first one of a series of videos that demo the various
DevMine tools.
&lt;/p&gt;
&lt;p&gt;
I suggest you watch it full screen for a better experience.
&lt;/p&gt;
&lt;p&gt;
&lt;a href=&quot;/doc/srctool&quot;&gt;&lt;code&gt;srctool&lt;/code&gt;&lt;/a&gt; is a command
line tool to manage source code parsers.
As such, it is able to install, delete or update parsers but moreover, it can
be used to parse projects sources by running the language parsers and merging
their output into one JSON.
&lt;/p&gt;
&lt;p&gt;
For more information about this tool, head to its dedicated
&lt;a href=&quot;/doc/srctool&quot;&gt;documentation page&lt;/a&gt;.
&lt;/p&gt;
&lt;iframe src=&quot;https://player.vimeo.com/video/126615350?color=337ab7&quot; width=&quot;800&quot; height=&quot;450&quot; frameborder=&quot;0&quot; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
</description>
        <pubDate>Fri, 01 May 2015 00:00:00 +0200</pubDate>
        <link>http://devmine.ch/news/2015/05/01/srctool-video/</link>
        <guid isPermaLink="true">http://devmine.ch/news/2015/05/01/srctool-video/</guid>
        
        
        <category>video</category>
        
      </item>
    
  </channel>
</rss>
